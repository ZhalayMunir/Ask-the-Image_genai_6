# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TVEYeEaMWYqobIFV5Imagkjk9GAJb8X0
"""

!pip install torch torchvision transformers gradio openai-whisper gtts Pillow

import whisper
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from gtts import gTTS
import torch
import gradio as gr
from PIL import Image
import tempfile
import os

# Device configuration
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load Whisper for ASR (Speech-to-text)
asr_model = whisper.load_model("small").to(device)

# !huggingface-cli login

# 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

# Load BLIP-2 for Image Question-Answering
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-opt-2.7b",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32
).to(device)

def speech_to_text(audio_file):
    if audio_file is None or not os.path.exists(audio_file):
        return "No audio received or file not found."

    # Whisper ASR transcription
    audio = whisper.load_audio(audio_file)
    audio = whisper.pad_or_trim(audio)
    mel = whisper.log_mel_spectrogram(audio).to(device)

    options = whisper.DecodingOptions(language="en")
    result = whisper.decode(asr_model, mel, options)

    return result.text


def image_qa(image, question):
    print("\nüîç [INPUT to VQA model]")
    print("Question:", question)

    # Use a properly formatted prompt
    prompt = f"Question: {question.strip()} Answer:"

    # ‚úÖ FIX: pass text as a list
    inputs = processor(images=image, text=[prompt], return_tensors="pt").to(
        device, torch.float16 if device == "cuda" else torch.float32
    )

    print("Tokenized input IDs:", inputs["input_ids"])

    generated_ids = model.generate(**inputs, max_new_tokens=50)

    print("\nüß† [Raw Output token IDs from model]")
    print(generated_ids)

    answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print("\n‚úÖ [Final Decoded Answer]")
    print("Answer:", answer)

    return answer





def text_to_speech(text):
    if not text or text.strip() == "":
        text = "No answer was generated."

    tts = gTTS(text=text, lang='en')
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
    tts.save(temp_file.name)
    return temp_file.name


def multimodal_pipeline(audio, image):
    # Speech-to-text
    question = speech_to_text(audio)

    # Image-QA
    img = Image.open(image).convert("RGB")
    answer = image_qa(img, question)

    # Text-to-speech
    audio_response = text_to_speech(answer)

    return question, answer, audio_response

# Gradio Interface
iface = gr.Interface(
    fn=multimodal_pipeline,
    inputs=[

        gr.Audio(sources=["microphone", "upload"], type="filepath", label="Ask or Upload your Question (audio)"),
        gr.Image(type="filepath", label="Upload an Image")
    ],
    outputs=[
        gr.Textbox(label="Transcribed Question"),
        gr.Textbox(label="Answer"),
        gr.Audio(label="Spoken Answer")
    ],
    title="üéôÔ∏èüñºÔ∏è Ask-the-Image Mini-App",
    description="Record a spoken question and upload an image. The AI answers your question about the image."
)

iface.launch(debug=True)